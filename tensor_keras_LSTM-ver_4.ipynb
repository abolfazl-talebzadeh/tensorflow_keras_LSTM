{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing method II\n",
    "# 7 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "8lpVdnId3iYM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "VSLcswUI3iYP"
   },
   "outputs": [],
   "source": [
    "politifact = pd.read_json(r'./data/politifact.json')\n",
    "snopes = pd.read_json(r'./data/snopes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "owxJjfnX3iYQ"
   },
   "outputs": [],
   "source": [
    "# column names\n",
    "poli_cols = politifact.columns.to_list()\n",
    "snop_cols = snopes.columns.to_list()\n",
    "\n",
    "# poli_row_count, poli_col_count = politifact.shape\n",
    "# snop_row_count, snop_col_count = snopes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "2xM1yD6v3iYQ"
   },
   "outputs": [],
   "source": [
    "common_cols = set(poli_cols) & set(snop_cols)\n",
    "\n",
    "td = pd.concat([snopes[list(common_cols)], politifact[list(common_cols)]])\n",
    "td = td.reindex(columns = ['doc', 'claim', 'factchecker', 'url', 'sources', 'topic', 'published' ,'label' ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBX7czZ43iYR",
    "outputId": "71859cfa-7ca1-4da8-cf57-e55ac9182294"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [03:07<00:00, 26.85s/it]\n"
     ]
    }
   ],
   "source": [
    "mt = {\n",
    "        '\\n':\" \",\n",
    "        \"'\":None,\n",
    "        '\"':None,\n",
    "        '“':None,\n",
    "        '”':None,\n",
    "        '’':None,\n",
    "        '‘':None\n",
    "    }\n",
    "for n in range(10):\n",
    "    mt[str(n)]=None\n",
    "\n",
    "for x in string.punctuation:\n",
    "    mt[x]=None\n",
    "cols = td.columns.to_list()\n",
    "cols.remove('label')\n",
    "\n",
    "# cleaning the first file\n",
    "for col in tqdm(cols):\n",
    "    for indx, x in enumerate(td[col]):\n",
    "        try:\n",
    "            td.loc[indx,col] = (x.translate(x.maketrans(mt))).lower()\n",
    "        except:\n",
    "            if x==None:\n",
    "                td.loc[indx,col] = \"\"\n",
    "            elif type(x) == list:\n",
    "                p = \" \".join(str(z) for z in x)\n",
    "                td.loc[indx,col] = p.translate(p.maketrans(mt)).lower()\n",
    "            else:\n",
    "                print(f\"column:{col}\\trow:{indx}\\tvalue: {x} is invalid\\tvalue type: {type(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "VBh43NDF3iYT"
   },
   "outputs": [],
   "source": [
    "td = td.dropna(subset=['doc','claim','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = ['false','true','half-true','mostly-true','barely-true','pants-fire','mixture']\n",
    "\n",
    "td['label']=td.label.map(lambda x: x.lower())\n",
    "td = td[td.label.isin(valid_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = td.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "jCUtLNtr3iYU"
   },
   "outputs": [],
   "source": [
    "one_hot_prep = [one_hot(series['claim']+' '+ series['doc'],10000) for _, series in td.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Avb_FXSq_jcE",
    "outputId": "e4e13772-e7a3-45ef-e7e2-8cc4bce89608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6955\n"
     ]
    }
   ],
   "source": [
    "temp = [len(x) for x in one_hot_prep]\n",
    "print(max(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "CPyK5DXF_wxJ"
   },
   "outputs": [],
   "source": [
    "sent_len = 300\n",
    "embedded_docs = pad_sequences(one_hot_prep,padding='pre',truncating='pre',maxlen=sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "E7T2E-9CBChI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 300, 100)          5000000   \n",
      "                                                                 \n",
      " spatial_dropout1d_3 (Spatia  (None, 300, 100)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 707       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,081,107\n",
      "Trainable params: 5,081,107\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# embedding_vector_fetures = 64\n",
    "# vocab_size = 5000\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size,embedding_vector_fetures,input_length= sent_len))\n",
    "# model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))#LSTM(100))\n",
    "# model.add(Dense(24, activation = 'softmax'))\n",
    "# model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "vocab_size=50000\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length= sent_len))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQJqrcPRDCnf",
    "outputId": "0d7a5546-8fa0-4ea3-c0bf-6c564e049c44"
   },
   "outputs": [],
   "source": [
    "X = np.array(embedded_docs)\n",
    "Y = pd.get_dummies(td['label']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16699, 300) (4175, 300) (16699, 7) (4175, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "522/522 [==============================] - 107s 202ms/step - loss: 1.8083 - accuracy: 0.2819 - val_loss: 1.8089 - val_accuracy: 0.2752\n",
      "Epoch 2/10\n",
      "522/522 [==============================] - 105s 202ms/step - loss: 1.7888 - accuracy: 0.2853 - val_loss: 1.8217 - val_accuracy: 0.2683\n",
      "Epoch 3/10\n",
      "522/522 [==============================] - 105s 201ms/step - loss: 1.7662 - accuracy: 0.2894 - val_loss: 1.8404 - val_accuracy: 0.2601\n",
      "Epoch 4/10\n",
      "522/522 [==============================] - 105s 201ms/step - loss: 1.7365 - accuracy: 0.3010 - val_loss: 1.8652 - val_accuracy: 0.2371\n",
      "Epoch 5/10\n",
      "522/522 [==============================] - 105s 201ms/step - loss: 1.6974 - accuracy: 0.3230 - val_loss: 1.8939 - val_accuracy: 0.2261\n",
      "Epoch 6/10\n",
      "522/522 [==============================] - 105s 201ms/step - loss: 1.6549 - accuracy: 0.3377 - val_loss: 1.9175 - val_accuracy: 0.2247\n",
      "Epoch 7/10\n",
      "522/522 [==============================] - 105s 202ms/step - loss: 1.6081 - accuracy: 0.3500 - val_loss: 1.9709 - val_accuracy: 0.2175\n",
      "Epoch 8/10\n",
      "522/522 [==============================] - 105s 201ms/step - loss: 1.5669 - accuracy: 0.3582 - val_loss: 2.0652 - val_accuracy: 0.2010\n",
      "Epoch 9/10\n",
      "522/522 [==============================] - 105s 201ms/step - loss: 1.5219 - accuracy: 0.3675 - val_loss: 2.1065 - val_accuracy: 0.2010\n",
      "Epoch 10/10\n",
      "522/522 [==============================] - 105s 201ms/step - loss: 1.4856 - accuracy: 0.3705 - val_loss: 2.1907 - val_accuracy: 0.2103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff5ca19f130>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1790, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 7, 22) and (None, 7) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfinal_y_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy,loss)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filep8si1ssn.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1790, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/ubuntu/miniconda3/envs/tf/lib/python3.9/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 7, 22) and (None, 7) are incompatible\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test,final_y_test)\n",
    "print(accuracy,loss)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
